@inproceedings{chavesrichter2021,
 title = {Look at that! {BERT} can be easily distracted from paying attention to morphosyntax},
 booktitle = {Proceedings of the Society for Computation in Linguistics},
 author = {Rui P. Chaves and Stephanie N. Richter},
 year = {2021},
 month = {February},
 day = {15},
 organization = {Society for Computation in Linguistics},
 volume = {4},
 pages = {28--38},
 address = {online},
 editor = {},
 url = {https://scholarworks.umass.edu/scil/vol4/iss1/4},
 doi = {10.7275/b92s-qd21},
 abstract = {Syntactic knowledge  involves  not  only  the ability  to  combine  words  and  phrases,  but also  the  capacity  to  relate  different  and  yet truth-preserving structural variations (e.g. passivization, inversion, topicalization, extraposition, clefting, etc.), as well as the ability to infer that these syntactic variations all adhere to common morphosyntactic rules,  like subject-verb agreement.   Although there is some evidence  that  BERT  has  rich  syntactic  knowledge, our adversarial approach suggests that it is not deployed in a robust and linguistically appropriate way. English BERT can be tricked to miss even quite simple syntactic generalizations, when compared with GPT-2, underscoring the need for stronger priors and for linguistically controlled experiments in evaluation.},
}